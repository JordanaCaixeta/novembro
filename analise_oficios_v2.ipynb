{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä An√°lise de Of√≠cios de Quebra de Sigilo - Pipeline Completo\n",
    "\n",
    "Pipeline robusto para normaliza√ß√£o, an√°lise e prepara√ß√£o de textos OCR de of√≠cios judiciais.\n",
    "\n",
    "**Stack utilizada:**\n",
    "- `ftfy` + `charset-normalizer`: Corre√ß√£o de encoding/unicode\n",
    "- `language-tool-python`: Valida√ß√£o ortogr√°fica pt-BR\n",
    "- `spaCy pt_core_news_lg`: Tokeniza√ß√£o e NER b√°sico\n",
    "- `scikit-learn`: TF-IDF e an√°lise de vocabul√°rio\n",
    "- `rapidfuzz`: Matching fuzzy de varia√ß√µes\n",
    "- APIs OpenAI/Claude: An√°lise sem√¢ntica profunda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o das depend√™ncias (executar uma vez)\n",
    "# !pip install ftfy charset-normalizer language-tool-python spacy rapidfuzz pandas openpyxl scikit-learn unidecode\n",
    "# !python -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configura√ß√£o e Carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURA√á√ÉO ===\n",
    "CAMINHO_EXCEL = 'oficios.xlsx'  # Altere para seu arquivo\n",
    "COLUNA_TEXTO = 'texto'          # Altere para o nome da coluna\n",
    "\n",
    "# Carregamento lazy das bibliotecas pesadas\n",
    "_spacy_nlp = None\n",
    "_language_tool = None\n",
    "\n",
    "def get_spacy():\n",
    "    \"\"\"Carrega spaCy sob demanda.\"\"\"\n",
    "    global _spacy_nlp\n",
    "    if _spacy_nlp is None:\n",
    "        import spacy\n",
    "        try:\n",
    "            _spacy_nlp = spacy.load('pt_core_news_lg')\n",
    "        except OSError:\n",
    "            print(\"Baixando modelo spaCy pt_core_news_lg...\")\n",
    "            import subprocess\n",
    "            subprocess.run(['python', '-m', 'spacy', 'download', 'pt_core_news_lg'])\n",
    "            _spacy_nlp = spacy.load('pt_core_news_lg')\n",
    "        print(\"‚úì spaCy carregado\")\n",
    "    return _spacy_nlp\n",
    "\n",
    "def get_language_tool():\n",
    "    \"\"\"Carrega LanguageTool sob demanda.\"\"\"\n",
    "    global _language_tool\n",
    "    if _language_tool is None:\n",
    "        import language_tool_python\n",
    "        _language_tool = language_tool_python.LanguageTool('pt-BR')\n",
    "        print(\"‚úì LanguageTool carregado\")\n",
    "    return _language_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_oficios(caminho: str, coluna: str) -> pd.DataFrame:\n",
    "    \"\"\"Carrega of√≠cios de Excel/CSV.\"\"\"\n",
    "    if caminho.endswith('.csv'):\n",
    "        df = pd.read_csv(caminho)\n",
    "    else:\n",
    "        df = pd.read_excel(caminho)\n",
    "    \n",
    "    if coluna not in df.columns:\n",
    "        print(f\"‚ùå Coluna '{coluna}' n√£o encontrada.\")\n",
    "        print(f\"Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    df['texto_original'] = df[coluna].fillna('').astype(str)\n",
    "    print(f\"‚úì Carregados {len(df)} documentos\")\n",
    "    print(f\"  Tamanho m√©dio: {df['texto_original'].str.len().mean():.0f} caracteres\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = carregar_oficios(CAMINHO_EXCEL, COLUNA_TEXTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Diagn√≥stico de Encoding e Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnosticar_encoding(texto: str) -> Dict:\n",
    "    \"\"\"Diagnostica problemas de encoding no texto.\"\"\"\n",
    "    problemas = []\n",
    "    \n",
    "    # Mojibake comum (UTF-8 lido como Latin-1)\n",
    "    mojibake_patterns = {\n",
    "        '√É¬°': '√°', '√É ': '√†', '√É¬¢': '√¢', '√É¬£': '√£',\n",
    "        '√É¬©': '√©', '√É¬™': '√™', '√É¬≠': '√≠',\n",
    "        '√É¬≥': '√≥', '√É¬¥': '√¥', '√É¬µ': '√µ',\n",
    "        '√É¬∫': '√∫', '√É¬ß': '√ß', '√É¬±': '√±',\n",
    "        '√É': '√Å', '√É‚Ä∞': '√â', '√É"': '√ì',\n",
    "        '√¢‚Ç¨≈ì': '\"', '√¢‚Ç¨': '\"', '√¢‚Ç¨‚Ñ¢': \"'\",\n",
    "        '√¢‚Ç¨"': '‚Äî', '√¢‚Ç¨"': '‚Äì',\n",
    "    }\n",
    "    \n",
    "    mojibake_encontrado = []\n",
    "    for errado, correto in mojibake_patterns.items():\n",
    "        if errado in texto:\n",
    "            mojibake_encontrado.append((errado, correto))\n",
    "    \n",
    "    if mojibake_encontrado:\n",
    "        problemas.append({\n",
    "            'tipo': 'mojibake',\n",
    "            'descricao': 'UTF-8 decodificado como Latin-1/Windows-1252',\n",
    "            'exemplos': mojibake_encontrado[:5]\n",
    "        })\n",
    "    \n",
    "    # Caracteres de controle\n",
    "    controle = re.findall(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', texto)\n",
    "    if controle:\n",
    "        problemas.append({\n",
    "            'tipo': 'caracteres_controle',\n",
    "            'descricao': 'Bytes de controle n√£o-imprim√≠veis',\n",
    "            'quantidade': len(controle)\n",
    "        })\n",
    "    \n",
    "    # Replacement character\n",
    "    replacements = texto.count('ÔøΩ')\n",
    "    if replacements:\n",
    "        problemas.append({\n",
    "            'tipo': 'replacement_char',\n",
    "            'descricao': 'Caractere de substitui√ß√£o Unicode (bytes inv√°lidos)',\n",
    "            'quantidade': replacements\n",
    "        })\n",
    "    \n",
    "    # Caracteres Unicode estranhos\n",
    "    estranhos = []\n",
    "    for char in set(texto):\n",
    "        try:\n",
    "            cat = unicodedata.category(char)\n",
    "            # Categorias suspeitas: Cn (n√£o atribu√≠do), Co (private use), Cs (surrogate)\n",
    "            if cat in ['Cn', 'Co', 'Cs']:\n",
    "                estranhos.append((char, hex(ord(char))))\n",
    "        except:\n",
    "            estranhos.append((char, 'erro'))\n",
    "    \n",
    "    if estranhos:\n",
    "        problemas.append({\n",
    "            'tipo': 'unicode_invalido',\n",
    "            'descricao': 'Caracteres Unicode inv√°lidos ou privados',\n",
    "            'exemplos': estranhos[:5]\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'tem_problemas': len(problemas) > 0,\n",
    "        'problemas': problemas,\n",
    "        'encoding_provavel': 'utf-8' if not problemas else 'latin-1/cp1252 mal convertido'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnosticar_quebras_linha(texto: str) -> Dict:\n",
    "    \"\"\"Analisa padr√µes de quebra de linha no texto.\"\"\"\n",
    "    stats = {\n",
    "        'unix_lf': texto.count('\\n') - texto.count('\\r\\n'),\n",
    "        'windows_crlf': texto.count('\\r\\n'),\n",
    "        'mac_cr': texto.count('\\r') - texto.count('\\r\\n'),\n",
    "        'form_feed': texto.count('\\f'),\n",
    "        'vertical_tab': texto.count('\\v'),\n",
    "    }\n",
    "    \n",
    "    # Detecta padr√£o predominante\n",
    "    if stats['windows_crlf'] > stats['unix_lf']:\n",
    "        padrao = 'Windows (CRLF)'\n",
    "    elif stats['mac_cr'] > 0:\n",
    "        padrao = 'Mac antigo (CR)'\n",
    "    else:\n",
    "        padrao = 'Unix (LF)'\n",
    "    \n",
    "    # Detecta hifeniza√ß√£o\n",
    "    hifenizacao = len(re.findall(r'\\w+-\\s*[\\r\\n]+\\s*\\w+', texto))\n",
    "    \n",
    "    # Linhas muito curtas (poss√≠vel OCR fragmentado)\n",
    "    linhas = texto.split('\\n')\n",
    "    linhas_curtas = sum(1 for l in linhas if 0 < len(l.strip()) < 15)\n",
    "    \n",
    "    return {\n",
    "        'estatisticas': stats,\n",
    "        'padrao_predominante': padrao,\n",
    "        'palavras_hifenizadas': hifenizacao,\n",
    "        'linhas_curtas': linhas_curtas,\n",
    "        'total_linhas': len(linhas),\n",
    "        'proporcao_curtas': linhas_curtas / max(len(linhas), 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_encoding_corpus(df: pd.DataFrame, coluna: str = 'texto_original') -> Dict:\n",
    "    \"\"\"Analisa problemas de encoding em todo o corpus.\"\"\"\n",
    "    resultados = {\n",
    "        'docs_com_mojibake': 0,\n",
    "        'docs_com_controle': 0,\n",
    "        'docs_com_replacement': 0,\n",
    "        'total_hifenizacao': 0,\n",
    "        'exemplos_mojibake': [],\n",
    "        'exemplos_hifenizacao': []\n",
    "    }\n",
    "    \n",
    "    for idx, texto in enumerate(df[coluna]):\n",
    "        enc = diagnosticar_encoding(texto)\n",
    "        quebras = diagnosticar_quebras_linha(texto)\n",
    "        \n",
    "        for prob in enc['problemas']:\n",
    "            if prob['tipo'] == 'mojibake':\n",
    "                resultados['docs_com_mojibake'] += 1\n",
    "                if len(resultados['exemplos_mojibake']) < 3:\n",
    "                    resultados['exemplos_mojibake'].append({\n",
    "                        'idx': idx,\n",
    "                        'exemplos': prob['exemplos']\n",
    "                    })\n",
    "            elif prob['tipo'] == 'caracteres_controle':\n",
    "                resultados['docs_com_controle'] += 1\n",
    "            elif prob['tipo'] == 'replacement_char':\n",
    "                resultados['docs_com_replacement'] += 1\n",
    "        \n",
    "        resultados['total_hifenizacao'] += quebras['palavras_hifenizadas']\n",
    "        if quebras['palavras_hifenizadas'] > 0 and len(resultados['exemplos_hifenizacao']) < 3:\n",
    "            matches = re.findall(r'(\\w+)-\\s*[\\r\\n]+\\s*(\\w+)', texto)[:2]\n",
    "            resultados['exemplos_hifenizacao'].append({\n",
    "                'idx': idx,\n",
    "                'exemplos': [f\"{m[0]}-{m[1]}\" for m in matches]\n",
    "            })\n",
    "    \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo:\n",
    "# encoding_stats = analisar_encoding_corpus(df)\n",
    "# print(f\"Docs com mojibake: {encoding_stats['docs_com_mojibake']}\")\n",
    "# print(f\"Total palavras hifenizadas: {encoding_stats['total_hifenizacao']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Normaliza√ß√£o Robusta de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_encoding(texto: str) -> str:\n",
    "    \"\"\"Corrige problemas de encoding usando ftfy.\"\"\"\n",
    "    try:\n",
    "        import ftfy\n",
    "        return ftfy.fix_text(texto)\n",
    "    except ImportError:\n",
    "        # Fallback manual para mojibake comum\n",
    "        correcoes = {\n",
    "            '√É¬°': '√°', '√É ': '√†', '√É¬¢': '√¢', '√É¬£': '√£',\n",
    "            '√É¬©': '√©', '√É¬™': '√™', '√É¬≠': '√≠',\n",
    "            '√É¬≥': '√≥', '√É¬¥': '√¥', '√É¬µ': '√µ',\n",
    "            '√É¬∫': '√∫', '√É¬ß': '√ß',\n",
    "            '√É': '√Å', '√É‚Ä∞': '√â', '√É"': '√ì', '√É≈°': '√ö',\n",
    "            '√É‚Ä°': '√á',\n",
    "        }\n",
    "        for errado, correto in correcoes.items():\n",
    "            texto = texto.replace(errado, correto)\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_quebras_linha(texto: str) -> str:\n",
    "    \"\"\"Normaliza quebras de linha e corrige hifeniza√ß√£o.\"\"\"\n",
    "    # Padroniza para \\n\n",
    "    texto = texto.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    \n",
    "    # Remove form feed e vertical tab\n",
    "    texto = texto.replace('\\f', '\\n').replace('\\v', '\\n')\n",
    "    \n",
    "    # Corrige hifeniza√ß√£o (palavra quebrada entre linhas)\n",
    "    # Padr√£o: palavra- \\n continua√ß√£o\n",
    "    texto = re.sub(\n",
    "        r'([a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ßA-Z√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á]+)-\\s*\\n\\s*([a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß]+)',\n",
    "        r'\\1\\2',\n",
    "        texto\n",
    "    )\n",
    "    \n",
    "    # Remove linhas em branco excessivas\n",
    "    texto = re.sub(r'\\n{3,}', '\\n\\n', texto)\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_artefatos_ocr(texto: str) -> str:\n",
    "    \"\"\"Remove artefatos comuns de OCR.\"\"\"\n",
    "    # Caracteres de controle (exceto \\n e \\t)\n",
    "    texto = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', '', texto)\n",
    "    \n",
    "    # Replacement character\n",
    "    texto = texto.replace('ÔøΩ', '')\n",
    "    \n",
    "    # Sequ√™ncias de pontua√ß√£o repetida (artefato comum)\n",
    "    texto = re.sub(r'([.,;:!?])\\1{2,}', r'\\1', texto)\n",
    "    \n",
    "    # Underscores ou h√≠fens repetidos (linhas decorativas)\n",
    "    texto = re.sub(r'[_\\-=]{5,}', '', texto)\n",
    "    \n",
    "    # Espa√ßos m√∫ltiplos\n",
    "    texto = re.sub(r' {2,}', ' ', texto)\n",
    "    \n",
    "    # Espa√ßos antes de pontua√ß√£o\n",
    "    texto = re.sub(r'\\s+([.,;:!?)])', r'\\1', texto)\n",
    "    \n",
    "    return texto.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto_completo(texto: str) -> str:\n",
    "    \"\"\"Pipeline completo de normaliza√ß√£o.\"\"\"\n",
    "    if not texto or len(texto.strip()) == 0:\n",
    "        return ''\n",
    "    \n",
    "    # 1. Corrige encoding\n",
    "    texto = normalizar_encoding(texto)\n",
    "    \n",
    "    # 2. Normaliza Unicode para NFC (composi√ß√£o can√¥nica)\n",
    "    texto = unicodedata.normalize('NFC', texto)\n",
    "    \n",
    "    # 3. Corrige quebras de linha e hifeniza√ß√£o\n",
    "    texto = normalizar_quebras_linha(texto)\n",
    "    \n",
    "    # 4. Remove artefatos de OCR\n",
    "    texto = remover_artefatos_ocr(texto)\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_corpus(df: pd.DataFrame, coluna: str = 'texto_original') -> pd.DataFrame:\n",
    "    \"\"\"Normaliza todos os textos do corpus.\"\"\"\n",
    "    print(\"Normalizando textos...\")\n",
    "    df['texto_normalizado'] = df[coluna].apply(normalizar_texto_completo)\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    tam_antes = df[coluna].str.len().sum()\n",
    "    tam_depois = df['texto_normalizado'].str.len().sum()\n",
    "    \n",
    "    print(f\"‚úì {len(df)} documentos normalizados\")\n",
    "    print(f\"  Redu√ß√£o: {(1 - tam_depois/tam_antes)*100:.1f}% (artefatos removidos)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = normalizar_corpus(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Avalia√ß√£o de Qualidade OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras portuguesas comuns para valida√ß√£o r√°pida (sem depend√™ncia externa)\n",
    "PALAVRAS_PT_COMUNS = {\n",
    "    'de', 'da', 'do', 'que', 'em', 'para', 'com', 'n√£o', 'uma', 'os', 'no', 'se',\n",
    "    'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi', 'ao', 'ele', 'das',\n",
    "    'tem', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito', 'h√°', 'nos', 'j√°',\n",
    "    'est√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©', 'isso', 'ela', 'entre',\n",
    "    'era', 'depois', 'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem', 'nas', 'me',\n",
    "    'esse', 'eles', 'est√£o', 'voc√™', 'tinha', 'foram', 'essa', 'num', 'nem',\n",
    "    'suas', 'meu', '√†s', 'minha', 't√™m', 'numa', 'pelos', 'elas', 'havia',\n",
    "    # Jur√≠dicas\n",
    "    'processo', 'juiz', 'tribunal', 'vara', 'comarca', 'of√≠cio', 'r√©u', 'autor',\n",
    "    'senten√ßa', 'decis√£o', 'recurso', 'prazo', 'dias', 'artigo', 'lei', 'c√≥digo',\n",
    "    'penal', 'civil', 'criminal', 'inqu√©rito', 'investiga√ß√£o', 'quebra', 'sigilo',\n",
    "    'banc√°rio', 'conta', 'extrato', 'banco', 'institui√ß√£o', 'financeira',\n",
    "    # Ita√∫\n",
    "    'ita√∫', 'itau', 'unibanco', 'itaucard', 'personnalit√©', 'bba'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas_qualidade(texto: str) -> Dict:\n",
    "    \"\"\"Calcula m√©tricas detalhadas de qualidade OCR.\"\"\"\n",
    "    if not texto or len(texto) < 50:\n",
    "        return {'score': 0, 'valido': False, 'motivo': 'texto muito curto'}\n",
    "    \n",
    "    metricas = {}\n",
    "    \n",
    "    # 1. Propor√ß√£o de caracteres alfab√©ticos vs total\n",
    "    chars_alpha = sum(1 for c in texto if c.isalpha())\n",
    "    chars_total = len(texto.replace(' ', '').replace('\\n', ''))\n",
    "    metricas['proporcao_alfabetica'] = chars_alpha / max(chars_total, 1)\n",
    "    \n",
    "    # 2. Tokens (palavras)\n",
    "    tokens = re.findall(r'\\b[a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ßA-Z√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á]+\\b', texto.lower())\n",
    "    metricas['total_tokens'] = len(tokens)\n",
    "    \n",
    "    # 3. Tamanho m√©dio de palavra\n",
    "    if tokens:\n",
    "        tamanhos = [len(t) for t in tokens]\n",
    "        metricas['tamanho_medio_palavra'] = sum(tamanhos) / len(tamanhos)\n",
    "        metricas['palavras_muito_curtas'] = sum(1 for t in tamanhos if t <= 2) / len(tamanhos)\n",
    "        metricas['palavras_muito_longas'] = sum(1 for t in tamanhos if t > 20) / len(tamanhos)\n",
    "    else:\n",
    "        metricas['tamanho_medio_palavra'] = 0\n",
    "        metricas['palavras_muito_curtas'] = 1\n",
    "        metricas['palavras_muito_longas'] = 0\n",
    "    \n",
    "    # 4. Propor√ß√£o de palavras conhecidas (portugu√™s)\n",
    "    palavras_conhecidas = sum(1 for t in tokens if t in PALAVRAS_PT_COMUNS)\n",
    "    metricas['proporcao_palavras_conhecidas'] = palavras_conhecidas / max(len(tokens), 1)\n",
    "    \n",
    "    # 5. Palavras com padr√µes suspeitos (muito consoantes seguidas, etc)\n",
    "    padrao_invalido = r'[bcdfghjklmnpqrstvwxyz]{5,}|[aeiou]{5,}'\n",
    "    palavras_suspeitas = sum(1 for t in tokens if re.search(padrao_invalido, t))\n",
    "    metricas['proporcao_suspeitas'] = palavras_suspeitas / max(len(tokens), 1)\n",
    "    \n",
    "    # 6. Caracteres especiais/ru√≠do\n",
    "    ruido = len(re.findall(r'[^a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ßA-Z√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á0-9\\s.,;:!?()\\[\\]\\-\"/\\n]', texto))\n",
    "    metricas['proporcao_ruido'] = ruido / max(len(texto), 1)\n",
    "    \n",
    "    # 7. N√∫meros v√°lidos (CPF, CNPJ, datas, processos)\n",
    "    cpfs = len(re.findall(r'\\d{3}\\.?\\d{3}\\.?\\d{3}-?\\d{2}', texto))\n",
    "    cnpjs = len(re.findall(r'\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2}', texto))\n",
    "    datas = len(re.findall(r'\\d{2}/\\d{2}/\\d{4}', texto))\n",
    "    metricas['documentos_estruturados'] = cpfs + cnpjs + datas\n",
    "    \n",
    "    # Score final (0-1)\n",
    "    score = (\n",
    "        metricas['proporcao_alfabetica'] * 0.15 +\n",
    "        min(metricas['proporcao_palavras_conhecidas'] * 2, 0.25) +  # at√© 0.25\n",
    "        (1 - metricas['proporcao_suspeitas']) * 0.20 +\n",
    "        (1 - metricas['proporcao_ruido'] * 10) * 0.15 +  # penaliza ru√≠do\n",
    "        (1 - metricas['palavras_muito_curtas']) * 0.10 +\n",
    "        (0.15 if 3.5 <= metricas['tamanho_medio_palavra'] <= 8 else 0.05)  # tamanho ideal\n",
    "    )\n",
    "    score = max(0, min(1, score))  # Clamp entre 0 e 1\n",
    "    \n",
    "    metricas['score'] = round(score, 3)\n",
    "    \n",
    "    # Classifica√ß√£o\n",
    "    if score >= 0.75:\n",
    "        metricas['qualidade'] = 'boa'\n",
    "    elif score >= 0.55:\n",
    "        metricas['qualidade'] = 'media'\n",
    "    elif score >= 0.35:\n",
    "        metricas['qualidade'] = 'baixa'\n",
    "    else:\n",
    "        metricas['qualidade'] = 'muito_baixa'\n",
    "    \n",
    "    metricas['requer_revisao'] = score < 0.55\n",
    "    \n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_qualidade_com_spellcheck(texto: str) -> Dict:\n",
    "    \"\"\"Avalia qualidade usando LanguageTool (mais preciso, mais lento).\"\"\"\n",
    "    tool = get_language_tool()\n",
    "    \n",
    "    # Limita texto para performance\n",
    "    texto_amostra = texto[:5000] if len(texto) > 5000 else texto\n",
    "    \n",
    "    erros = tool.check(texto_amostra)\n",
    "    \n",
    "    # Filtra erros relevantes (ignora estilo, foca em ortografia)\n",
    "    erros_ortografia = [e for e in erros if 'SPELL' in e.ruleId or 'TYPO' in e.ruleId]\n",
    "    \n",
    "    palavras = len(texto_amostra.split())\n",
    "    taxa_erro = len(erros_ortografia) / max(palavras, 1)\n",
    "    \n",
    "    return {\n",
    "        'total_erros': len(erros),\n",
    "        'erros_ortografia': len(erros_ortografia),\n",
    "        'taxa_erro': taxa_erro,\n",
    "        'score_spellcheck': max(0, 1 - taxa_erro * 5),  # 20% erro = score 0\n",
    "        'exemplos_erros': [{'texto': e.context, 'sugestao': e.replacements[:2]} \n",
    "                          for e in erros_ortografia[:5]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_qualidade_corpus(df: pd.DataFrame, coluna: str = 'texto_normalizado', \n",
    "                              usar_spellcheck: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Avalia qualidade de todo o corpus.\"\"\"\n",
    "    print(\"Avaliando qualidade OCR...\")\n",
    "    \n",
    "    # M√©tricas b√°sicas (r√°pido)\n",
    "    metricas = df[coluna].apply(calcular_metricas_qualidade)\n",
    "    df_metricas = pd.DataFrame(metricas.tolist())\n",
    "    \n",
    "    # Adiciona ao DataFrame\n",
    "    for col in ['score', 'qualidade', 'requer_revisao', 'total_tokens']:\n",
    "        if col in df_metricas.columns:\n",
    "            df[f'ocr_{col}'] = df_metricas[col]\n",
    "    \n",
    "    # Spellcheck (opcional, lento)\n",
    "    if usar_spellcheck:\n",
    "        print(\"  Executando spellcheck (pode demorar)...\")\n",
    "        spell_results = df[coluna].apply(avaliar_qualidade_com_spellcheck)\n",
    "        df['ocr_score_spell'] = [r['score_spellcheck'] for r in spell_results]\n",
    "    \n",
    "    # Resumo\n",
    "    print(f\"\\n‚úì An√°lise conclu√≠da:\")\n",
    "    print(f\"  Score m√©dio: {df['ocr_score'].mean():.2f}\")\n",
    "    print(f\"  Distribui√ß√£o de qualidade:\")\n",
    "    print(df['ocr_qualidade'].value_counts().to_string())\n",
    "    print(f\"\\n  Documentos para revis√£o: {df['ocr_requer_revisao'].sum()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = analisar_qualidade_corpus(df, usar_spellcheck=False)  # True para an√°lise mais precisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Mapeamento Completo do Grupo Ita√∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTIDADES_ITAU = {\n",
    "    # === PRINCIPAIS ===\n",
    "    'itau_unibanco_holding': {\n",
    "        'cnpj': '60.872.504/0001-23',\n",
    "        'nomes': ['ITA√ö UNIBANCO HOLDING', 'ITAU UNIBANCO HOLDING', 'ITA√ö HOLDING'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    'itau_unibanco': {\n",
    "        'cnpj': '60.701.190/0001-04',\n",
    "        'nomes': ['ITA√ö UNIBANCO', 'ITAU UNIBANCO', 'BANCO ITA√ö', 'BANCO ITAU', \n",
    "                  'ITA√ö S.A.', 'ITAU S.A.', 'ITA√ö SA', 'ITAU SA'],\n",
    "        'codigo_banco': '341'\n",
    "    },\n",
    "    \n",
    "    # === SEGMENTOS ===\n",
    "    'itau_bba': {\n",
    "        'cnpj': '17.298.092/0001-30',\n",
    "        'nomes': ['ITA√ö BBA', 'ITAU BBA', 'BANCO ITA√ö BBA', 'BBA'],\n",
    "        'codigo_banco': '184'\n",
    "    },\n",
    "    'itaucard': {\n",
    "        'cnpj': '17.192.451/0001-70',\n",
    "        'nomes': ['ITAUCARD', 'BANCO ITAUCARD', 'ITA√ö CARD', 'ITAU CARD'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    'itau_consignado': {\n",
    "        'cnpj': '33.885.724/0001-19',\n",
    "        'nomes': ['ITA√ö CONSIGNADO', 'ITAU CONSIGNADO', 'BANCO ITA√ö CONSIGNADO',\n",
    "                  'ITA√ö BMG CONSIGNADO', 'ITAU BMG'],\n",
    "        'codigo_banco': '029'\n",
    "    },\n",
    "    'itau_personnalite': {\n",
    "        'cnpj': None,  # Usa mesmo CNPJ do Ita√∫ Unibanco\n",
    "        'nomes': ['PERSONNALIT√â', 'PERSONNALITE', 'ITA√ö PERSONNALIT√â', \n",
    "                  'ITAU PERSONNALITE', 'ITA√ö PERSONALITE'],\n",
    "        'codigo_banco': '341'\n",
    "    },\n",
    "    'itau_private': {\n",
    "        'cnpj': None,\n",
    "        'nomes': ['ITA√ö PRIVATE', 'ITAU PRIVATE', 'PRIVATE BANK ITA√ö'],\n",
    "        'codigo_banco': '341'\n",
    "    },\n",
    "    'iti': {\n",
    "        'cnpj': None,  # Usa infraestrutura Ita√∫\n",
    "        'nomes': ['ITI', 'BANCO ITI', 'ITI ITA√ö'],\n",
    "        'codigo_banco': '341'\n",
    "    },\n",
    "    \n",
    "    # === SEGUROS E PREVID√äNCIA ===\n",
    "    'itau_seguros': {\n",
    "        'cnpj': '61.557.039/0001-07',\n",
    "        'nomes': ['ITA√ö SEGUROS', 'ITAU SEGUROS'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    'itau_vida_previdencia': {\n",
    "        'cnpj': '92.661.388/0001-90',\n",
    "        'nomes': ['ITA√ö VIDA E PREVID√äNCIA', 'ITAU VIDA E PREVIDENCIA',\n",
    "                  'ITA√ö PREVID√äNCIA', 'ITAU PREVIDENCIA'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    \n",
    "    # === FINANCEIRAS E CR√âDITO ===\n",
    "    'financeira_itau_cbd': {\n",
    "        'cnpj': '06.881.898/0001-30',\n",
    "        'nomes': ['FINANCEIRA ITA√ö CBD', 'FIC', 'ITA√ö CBD'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    'luizacred': {\n",
    "        'cnpj': '02.206.577/0001-80',\n",
    "        'nomes': ['LUIZACRED', 'MAGAZINE LUIZA ITA√ö'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    \n",
    "    # === CART√ïES ===\n",
    "    'hipercard': {\n",
    "        'cnpj': '03.012.230/0001-69',\n",
    "        'nomes': ['HIPERCARD', 'HIPER CARD'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    'credicard': {\n",
    "        'cnpj': '03.766.873/0001-06',\n",
    "        'nomes': ['CREDICARD', 'CREDI CARD'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    'rede': {\n",
    "        'cnpj': '01.425.787/0001-04',\n",
    "        'nomes': ['REDE', 'REDECARD', 'REDE ITA√ö'],\n",
    "        'codigo_banco': None\n",
    "    },\n",
    "    \n",
    "    # === BANCOS ADQUIRIDOS (podem aparecer em processos antigos) ===\n",
    "    'unibanco': {\n",
    "        'cnpj': '33.700.394/0001-40',\n",
    "        'nomes': ['UNIBANCO', 'UNI√ÉO DE BANCOS BRASILEIROS'],\n",
    "        'codigo_banco': '409',\n",
    "        'nota': 'Fus√£o com Ita√∫ em 2008'\n",
    "    },\n",
    "    'banco_nacional': {\n",
    "        'cnpj': None,\n",
    "        'nomes': ['BANCO NACIONAL', 'NACIONAL'],\n",
    "        'codigo_banco': None,\n",
    "        'nota': 'Adquirido em 1995'\n",
    "    },\n",
    "    'bankboston': {\n",
    "        'cnpj': None,\n",
    "        'nomes': ['BANKBOSTON', 'BANK BOSTON', 'BOSTON'],\n",
    "        'codigo_banco': None,\n",
    "        'nota': 'Adquirido em 2006'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varia√ß√µes de grafia por erro de OCR\n",
    "VARIACOES_OCR_ITAU = [\n",
    "    # Sem acento\n",
    "    'ITAU', 'itau', 'Itau',\n",
    "    # Com acento correto\n",
    "    'ITA√ö', 'ita√∫', 'Ita√∫',\n",
    "    # Acentos errados comuns\n",
    "    'ITA√ô', 'ITA√î', 'ITA√ì', 'IT√ÇU', '√åTA√ö', '√çTA√ö',\n",
    "    # Substitui√ß√µes OCR (0 por O, 1 por I/l)\n",
    "    'ITA0', 'ITAO', '1TA√ö', '1TAU', 'lTA√ö', 'lTAU',\n",
    "    'ITA√ú',  # trema por acento\n",
    "    # Espa√ßamento incorreto\n",
    "    'I TA√ö', 'IT A√ö', 'ITA √ö',\n",
    "    'I T A √ö',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_regex_itau() -> str:\n",
    "    \"\"\"Cria regex abrangente para detectar men√ß√µes ao Ita√∫.\"\"\"\n",
    "    # Padr√£o base: ITA + vogal (com poss√≠veis erros OCR)\n",
    "    # Seguido opcionalmente de segmento\n",
    "    segmentos = '(?:UNIBANCO|BBA|CARD|CONSIGNADO|SEGUROS|PERSONNALITE|PERSONNALIT√â|PRIVATE|BMG)?'\n",
    "    \n",
    "    # Aceita varia√ß√µes de acentua√ß√£o e erros OCR comuns\n",
    "    base = r'[Il1][Tt][Aa][U√ö√ô√î√ì√úu√∫√π√¥√≥√º0O]'\n",
    "    \n",
    "    return rf'\\b{base}[\\s\\-]*{segmentos}\\b'\n",
    "\n",
    "REGEX_ITAU = re.compile(criar_regex_itau(), re.IGNORECASE)\n",
    "\n",
    "# CNPJs do grupo\n",
    "CNPJS_ITAU = [\n",
    "    r'60\\.?872\\.?504/?0001-?23',  # Holding\n",
    "    r'60\\.?701\\.?190/?0001-?04',  # Unibanco\n",
    "    r'17\\.?298\\.?092/?0001-?30',  # BBA\n",
    "    r'17\\.?192\\.?451/?0001-?70',  # Itaucard\n",
    "    r'33\\.?885\\.?724/?0001-?19',  # Consignado\n",
    "    r'61\\.?557\\.?039/?0001-?07',  # Seguros\n",
    "    r'92\\.?661\\.?388/?0001-?90',  # Vida e Previd√™ncia\n",
    "    r'06\\.?881\\.?898/?0001-?30',  # Financeira CBD\n",
    "    r'02\\.?206\\.?577/?0001-?80',  # Luizacred\n",
    "    r'03\\.?012\\.?230/?0001-?69',  # Hipercard\n",
    "    r'03\\.?766\\.?873/?0001-?06',  # Credicard\n",
    "    r'01\\.?425\\.?787/?0001-?04',  # Rede\n",
    "    r'33\\.?700\\.?394/?0001-?40',  # Unibanco (hist√≥rico)\n",
    "]\n",
    "\n",
    "REGEX_CNPJ_ITAU = re.compile('|'.join(CNPJS_ITAU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_mencoes_itau(texto: str) -> Dict:\n",
    "    \"\"\"Detecta todas as men√ß√µes ao Ita√∫ no texto.\"\"\"\n",
    "    resultado = {\n",
    "        'menciona_itau': False,\n",
    "        'mencoes_nome': [],\n",
    "        'mencoes_cnpj': [],\n",
    "        'entidades_identificadas': set(),\n",
    "        'codigo_banco_341': '341' in texto\n",
    "    }\n",
    "    \n",
    "    # Busca por nome\n",
    "    for match in REGEX_ITAU.finditer(texto):\n",
    "        mencao = match.group().strip().upper()\n",
    "        ctx_inicio = max(0, match.start() - 30)\n",
    "        ctx_fim = min(len(texto), match.end() + 30)\n",
    "        \n",
    "        resultado['mencoes_nome'].append({\n",
    "            'texto': mencao,\n",
    "            'contexto': texto[ctx_inicio:ctx_fim].strip(),\n",
    "            'posicao': match.start()\n",
    "        })\n",
    "        \n",
    "        # Identifica entidade espec√≠fica\n",
    "        for entidade, dados in ENTIDADES_ITAU.items():\n",
    "            for nome in dados['nomes']:\n",
    "                if nome.upper() in mencao.upper() or mencao.upper() in nome.upper():\n",
    "                    resultado['entidades_identificadas'].add(entidade)\n",
    "    \n",
    "    # Busca por CNPJ\n",
    "    for match in REGEX_CNPJ_ITAU.finditer(texto):\n",
    "        cnpj = match.group()\n",
    "        resultado['mencoes_cnpj'].append(cnpj)\n",
    "        \n",
    "        # Identifica entidade pelo CNPJ\n",
    "        cnpj_limpo = re.sub(r'[^\\d]', '', cnpj)\n",
    "        for entidade, dados in ENTIDADES_ITAU.items():\n",
    "            if dados['cnpj']:\n",
    "                cnpj_entidade = re.sub(r'[^\\d]', '', dados['cnpj'])\n",
    "                if cnpj_limpo == cnpj_entidade:\n",
    "                    resultado['entidades_identificadas'].add(entidade)\n",
    "    \n",
    "    resultado['menciona_itau'] = len(resultado['mencoes_nome']) > 0 or len(resultado['mencoes_cnpj']) > 0\n",
    "    resultado['entidades_identificadas'] = list(resultado['entidades_identificadas'])\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_mencoes_itau_corpus(df: pd.DataFrame, coluna: str = 'texto_normalizado') -> Dict:\n",
    "    \"\"\"Analisa men√ß√µes ao Ita√∫ em todo o corpus.\"\"\"\n",
    "    resultados = []\n",
    "    for texto in df[coluna]:\n",
    "        resultados.append(detectar_mencoes_itau(texto))\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    docs_com_itau = sum(1 for r in resultados if r['menciona_itau'])\n",
    "    todas_mencoes = [m['texto'] for r in resultados for m in r['mencoes_nome']]\n",
    "    todas_entidades = [e for r in resultados for e in r['entidades_identificadas']]\n",
    "    \n",
    "    return {\n",
    "        'docs_com_itau': docs_com_itau,\n",
    "        'docs_sem_itau': len(df) - docs_com_itau,\n",
    "        'variacoes_nome': Counter(todas_mencoes).most_common(20),\n",
    "        'entidades_frequentes': Counter(todas_entidades).most_common(),\n",
    "        'detalhes': resultados\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultado_itau = analisar_mencoes_itau_corpus(df)\n",
    "# print(f\"Docs mencionando Ita√∫: {resultado_itau['docs_com_itau']}\")\n",
    "# print(f\"Varia√ß√µes encontradas: {resultado_itau['variacoes_nome'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. An√°lise de Vocabul√°rio e N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_PT = {\n",
    "    'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', '√©', 'com', 'n√£o',\n",
    "    'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi',\n",
    "    'ao', 'ele', 'das', 'tem', '√†', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito',\n",
    "    'h√°', 'nos', 'j√°', 'est√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©', 'isso',\n",
    "    'ela', 'entre', 'era', 'depois', 'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem',\n",
    "    'nas', 'me', 'esse', 'eles', 'est√£o', 'voc√™', 'tinha', 'foram', 'essa', 'num',\n",
    "    'nem', 'suas', 'meu', '√†s', 'minha', 't√™m', 'numa', 'pelos', 'elas', 'havia',\n",
    "    'seja', 'qual', 'ser√°', 'n√≥s', 'tenho', 'lhe', 'deles', 'essas', 'esses',\n",
    "    'pelas', 'este', 'fosse', 'dele', 'tu', 'te', 'voc√™s', 'vos', 'lhes', 'meus',\n",
    "    'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas',\n",
    "    'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles',\n",
    "    'aquelas', 'isto', 'aquilo', 'estou', 'est√°', 'estamos', 'est√£o', 'estive',\n",
    "    'esteve', 'estivemos', 'estiveram', 'estava', 'est√°vamos', 'estavam',\n",
    "    'sobre', 's√£o', 'the', 'and', 'of', 'to', 'in',  # Ingl√™s comum em docs\n",
    "}\n",
    "\n",
    "STOPWORDS_JURIDICAS = {\n",
    "    'artigo', 'art', 'par√°grafo', 'inciso', 'al√≠nea', 'caput', 'lei', 'c√≥digo',\n",
    "    'conforme', 'acordo', 'termo', 'presente', 'assim', 'ainda', 'portanto',\n",
    "    'todavia', 'contudo', 'entretanto', 'outrossim', 'destarte', 'ademais',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_texto(texto: str, remover_stopwords: bool = True) -> List[str]:\n",
    "    \"\"\"Tokeniza texto em palavras.\"\"\"\n",
    "    # Extrai palavras (incluindo acentuadas)\n",
    "    tokens = re.findall(r'\\b[a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ßA-Z√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á]{2,}\\b', texto.lower())\n",
    "    \n",
    "    if remover_stopwords:\n",
    "        stopwords = STOPWORDS_PT | STOPWORDS_JURIDICAS\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_ngramas(tokens: List[str], n: int = 2) -> List[str]:\n",
    "    \"\"\"Extrai n-gramas de uma lista de tokens.\"\"\"\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_vocabulario_corpus(df: pd.DataFrame, coluna: str = 'texto_normalizado') -> Dict:\n",
    "    \"\"\"An√°lise completa de vocabul√°rio do corpus.\"\"\"\n",
    "    print(\"Analisando vocabul√°rio...\")\n",
    "    \n",
    "    todos_tokens = []\n",
    "    todos_bigramas = []\n",
    "    todos_trigramas = []\n",
    "    \n",
    "    for texto in df[coluna]:\n",
    "        tokens = tokenizar_texto(texto)\n",
    "        todos_tokens.extend(tokens)\n",
    "        todos_bigramas.extend(extrair_ngramas(tokens, 2))\n",
    "        todos_trigramas.extend(extrair_ngramas(tokens, 3))\n",
    "    \n",
    "    return {\n",
    "        'vocabulario_unico': len(set(todos_tokens)),\n",
    "        'total_tokens': len(todos_tokens),\n",
    "        'palavras_frequentes': Counter(todos_tokens).most_common(50),\n",
    "        'bigramas_frequentes': Counter(todos_bigramas).most_common(30),\n",
    "        'trigramas_frequentes': Counter(todos_trigramas).most_common(20),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_tfidf(df: pd.DataFrame, coluna: str = 'texto_normalizado', \n",
    "                   top_n: int = 30) -> Dict:\n",
    "    \"\"\"An√°lise TF-IDF para encontrar termos distintivos.\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    stopwords = list(STOPWORDS_PT | STOPWORDS_JURIDICAS)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words=stopwords,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        token_pattern=r'\\b[a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ßA-Z√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á]{2,}\\b'\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(df[coluna])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Termos mais importantes (m√©dia TF-IDF)\n",
    "    mean_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "    top_indices = mean_tfidf.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    termos_importantes = [\n",
    "        (feature_names[i], round(mean_tfidf[i], 4))\n",
    "        for i in top_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'termos_tfidf': termos_importantes,\n",
    "        'total_features': len(feature_names),\n",
    "        'matriz_shape': tfidf_matrix.shape\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = analisar_vocabulario_corpus(df)\n",
    "# print(f\"Vocabul√°rio √∫nico: {vocab['vocabulario_unico']} palavras\")\n",
    "# print(f\"\\nBigramas mais frequentes:\")\n",
    "# for bigrama, freq in vocab['bigramas_frequentes'][:10]:\n",
    "#     print(f\"  '{bigrama}': {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Detec√ß√£o de Padr√µes Jur√≠dicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADROES_JURIDICOS = {\n",
    "    'processo_cnj': {\n",
    "        'regex': r'\\d{7}-\\d{2}\\.\\d{4}\\.\\d\\.\\d{2}\\.\\d{4}',\n",
    "        'descricao': 'N√∫mero CNJ (NNNNNNN-DD.AAAA.J.TR.OOOO)'\n",
    "    },\n",
    "    'processo_antigo': {\n",
    "        'regex': r'\\d{4,7}[/\\-]\\d{2,4}',\n",
    "        'descricao': 'N√∫mero processo formato antigo'\n",
    "    },\n",
    "    'cpf': {\n",
    "        'regex': r'\\d{3}\\.?\\d{3}\\.?\\d{3}[-.]?\\d{2}',\n",
    "        'descricao': 'CPF'\n",
    "    },\n",
    "    'cnpj': {\n",
    "        'regex': r'\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}[-.]?\\d{2}',\n",
    "        'descricao': 'CNPJ'\n",
    "    },\n",
    "    'data_br': {\n",
    "        'regex': r'\\d{2}/\\d{2}/\\d{4}',\n",
    "        'descricao': 'Data formato brasileiro'\n",
    "    },\n",
    "    'data_extenso': {\n",
    "        'regex': r'\\d{1,2}\\s+de\\s+(?:janeiro|fevereiro|mar√ßo|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\s+de\\s+\\d{4}',\n",
    "        'descricao': 'Data por extenso'\n",
    "    },\n",
    "    'valor_monetario': {\n",
    "        'regex': r'R\\$\\s*[\\d.,]+',\n",
    "        'descricao': 'Valor em reais'\n",
    "    },\n",
    "    'artigo_lei': {\n",
    "        'regex': r'[Aa]rt(?:igo)?[.]?\\s*\\d+',\n",
    "        'descricao': 'Refer√™ncia a artigo de lei'\n",
    "    },\n",
    "    'lei_referencia': {\n",
    "        'regex': r'[Ll]ei\\s*(?:[Nn][¬∫¬∞o]?\\.?)?\\s*[\\d.,/]+',\n",
    "        'descricao': 'Refer√™ncia a lei'\n",
    "    },\n",
    "    'oficio_numero': {\n",
    "        'regex': r'[Oo]f[i√≠]cio\\s*(?:[Nn][¬∫¬∞o]?\\.?)?\\s*[\\d/\\-]+',\n",
    "        'descricao': 'N√∫mero de of√≠cio'\n",
    "    },\n",
    "    'prazo_dias': {\n",
    "        'regex': r'(?:prazo\\s+(?:de\\s+)?)?\\d+\\s*(?:dias?|horas?)\\s*(?:√∫teis|corridos)?',\n",
    "        'descricao': 'Prazo em dias/horas'\n",
    "    },\n",
    "    'vara_judicial': {\n",
    "        'regex': r'\\d+[¬™a¬∫]?\\s*[Vv]ara\\s+(?:[Cc]riminal|[Cc][i√≠]vel|[Ff]ederal|[Dd]o\\s+[Tt]rabalho)',\n",
    "        'descricao': 'Vara judicial'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_padroes_juridicos(texto: str) -> Dict:\n",
    "    \"\"\"Extrai todos os padr√µes jur√≠dicos de um texto.\"\"\"\n",
    "    resultado = {}\n",
    "    \n",
    "    for nome, config in PADROES_JURIDICOS.items():\n",
    "        matches = re.findall(config['regex'], texto, re.IGNORECASE)\n",
    "        resultado[nome] = {\n",
    "            'quantidade': len(matches),\n",
    "            'exemplos': list(set(matches))[:5]\n",
    "        }\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_padroes_corpus(df: pd.DataFrame, coluna: str = 'texto_normalizado') -> Dict:\n",
    "    \"\"\"Analisa padr√µes jur√≠dicos em todo o corpus.\"\"\"\n",
    "    contagens = {nome: 0 for nome in PADROES_JURIDICOS}\n",
    "    exemplos = {nome: [] for nome in PADROES_JURIDICOS}\n",
    "    \n",
    "    for texto in df[coluna]:\n",
    "        padroes = extrair_padroes_juridicos(texto)\n",
    "        for nome, dados in padroes.items():\n",
    "            contagens[nome] += dados['quantidade']\n",
    "            if len(exemplos[nome]) < 10:\n",
    "                exemplos[nome].extend(dados['exemplos'])\n",
    "    \n",
    "    return {\n",
    "        'contagens': dict(sorted(contagens.items(), key=lambda x: x[1], reverse=True)),\n",
    "        'exemplos': {k: list(set(v))[:5] for k, v in exemplos.items()}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padroes = analisar_padroes_corpus(df)\n",
    "# print(\"Padr√µes encontrados:\")\n",
    "# for nome, qtd in padroes['contagens'].items():\n",
    "#     print(f\"  {nome}: {qtd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Classifica√ß√£o de Tipo de Of√≠cio e Se√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICADORES_TIPO_OFICIO = {\n",
    "    'primeiro_pedido': [\n",
    "        r'determino\\s+(?:a\\s+)?quebra',\n",
    "        r'defiro\\s+(?:o\\s+)?pedido',\n",
    "        r'decido\\s+pela\\s+quebra',\n",
    "        r'oficie-se\\s+(?:ao|√†)',\n",
    "        r'instaura[√ßc][√£a]o',\n",
    "        r'decreto\\s+(?:a\\s+)?quebra',\n",
    "    ],\n",
    "    'reiteracao': [\n",
    "        r'reitera[√ßc][√£a]o',\n",
    "        r'reitero\\s+(?:o\\s+)?(?:pedido|of√≠cio)',\n",
    "        r'novamente\\s+(?:solicito|requeiro)',\n",
    "        r'ainda\\s+(?:n√£o\\s+)?(?:foi\\s+)?atendid[oa]',\n",
    "        r'sem\\s+resposta',\n",
    "        r'prazo\\s+(?:j√°\\s+)?(?:esgotado|vencido)',\n",
    "        r'aguardando\\s+cumprimento',\n",
    "        r'insistimos',\n",
    "    ],\n",
    "    'complemento': [\n",
    "        r'complement(?:o|a[√ßc][√£a]o)',\n",
    "        r'adicionalmente',\n",
    "        r'al√©m\\s+d[oa]s?\\s+(?:j√°\\s+)?solicitad[oa]s?',\n",
    "        r'acrescento',\n",
    "        r'amplia[√ßc][√£a]o\\s+(?:do\\s+)?pedido',\n",
    "        r'novo\\s+per[i√≠]odo',\n",
    "        r'inclus[√£a]o\\s+de',\n",
    "        r'estendo\\s+(?:a\\s+)?quebra',\n",
    "    ],\n",
    "    'resposta': [\n",
    "        r'em\\s+resposta',\n",
    "        r'em\\s+atendimento',\n",
    "        r'cumprimento\\s+(?:ao|do)\\s+of√≠cio',\n",
    "        r'informamos\\s+que',\n",
    "        r'encaminhamos\\s+(?:em\\s+)?anexo',\n",
    "        r'seguem\\s+(?:as\\s+)?informa[√ßc][√µo]es',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificar_tipo_oficio(texto: str) -> Dict:\n",
    "    \"\"\"Classifica o tipo de of√≠cio baseado em indicadores.\"\"\"\n",
    "    texto_lower = texto.lower()\n",
    "    scores = {}\n",
    "    evidencias = {}\n",
    "    \n",
    "    for tipo, padroes in INDICADORES_TIPO_OFICIO.items():\n",
    "        matches = []\n",
    "        for p in padroes:\n",
    "            encontrados = re.findall(p, texto_lower)\n",
    "            matches.extend(encontrados)\n",
    "        scores[tipo] = len(matches)\n",
    "        evidencias[tipo] = matches[:3]\n",
    "    \n",
    "    # Determina tipo principal\n",
    "    if max(scores.values()) == 0:\n",
    "        tipo_principal = 'indeterminado'\n",
    "        confianca = 'baixa'\n",
    "    else:\n",
    "        tipo_principal = max(scores, key=scores.get)\n",
    "        max_score = scores[tipo_principal]\n",
    "        confianca = 'alta' if max_score >= 3 else 'media' if max_score >= 1 else 'baixa'\n",
    "    \n",
    "    return {\n",
    "        'tipo': tipo_principal,\n",
    "        'confianca': confianca,\n",
    "        'scores': scores,\n",
    "        'evidencias': evidencias.get(tipo_principal, [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICADORES_SECAO = {\n",
    "    'header_ocr': [\n",
    "        r'scanned\\s*by', r'p√°gina\\s*\\d+', r'page\\s*\\d+',\n",
    "        r'^[_\\-=]{5,}$', r'digitalizado',\n",
    "    ],\n",
    "    'header_juridico': [\n",
    "        r'poder\\s*judici[a√°]rio', r'tribunal', r'justi[c√ß]a',\n",
    "        r'comarca', r'vara', r'f[o√≥]rum', r'minist[e√©]rio\\s*p[u√∫]blico',\n",
    "        r'delegacia', r'pol[i√≠]cia',\n",
    "    ],\n",
    "    'identificacao': [\n",
    "        r'processo\\s*(?:n[¬∫¬∞])?', r'of[i√≠]cio\\s*(?:n[¬∫¬∞])?',\n",
    "        r'inqu[e√©]rito', r'procedimento', r'auto\\s*n',\n",
    "    ],\n",
    "    'fundamentacao': [\n",
    "        r'\\bart\\.?\\s*\\d+', r'lei\\s*(?:n[¬∫¬∞])?', r'c[o√≥]digo',\n",
    "        r'constitui[√ßc][√£a]o', r'jurisprud[e√™]ncia', r'entendimento',\n",
    "        r'considera(?:ndo)?', r'tendo\\s+em\\s+vista',\n",
    "    ],\n",
    "    'solicitacao': [\n",
    "        r'oficie-se', r'determino', r'defiro', r'requisite-se',\n",
    "        r'solicito', r'requeiro', r'forne√ßa', r'encaminhe',\n",
    "        r'apresente', r'informe', r'disponibilize',\n",
    "    ],\n",
    "    'investigado': [\n",
    "        r'\\d{3}\\.?\\d{3}\\.?\\d{3}-?\\d{2}',  # CPF\n",
    "        r'investigad[oa]', r'r[e√©]u', r'denunciad[oa]', r'indicia[oa]',\n",
    "        r'qualifica[√ßc][√£a]o', r'titular',\n",
    "    ],\n",
    "    'prazo': [\n",
    "        r'prazo\\s+(?:de\\s+)?\\d+', r'no\\s+prazo',\n",
    "        r'em\\s+\\d+\\s+dias', r'improrrog[a√°]vel',\n",
    "    ],\n",
    "    'rodape': [\n",
    "        r'documento\\s*assinado', r'assinatura\\s*(?:digital|eletr)',\n",
    "        r'certificado\\s*digital', r'c[o√≥]digo\\s*de\\s*verifica',\n",
    "        r'validar.*documento', r'\\.jus\\.br',\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificar_paragrafo(paragrafo: str) -> str:\n",
    "    \"\"\"Classifica um par√°grafo em uma categoria.\"\"\"\n",
    "    paragrafo_lower = paragrafo.lower().strip()\n",
    "    \n",
    "    if len(paragrafo_lower) < 10:\n",
    "        return 'ruido'\n",
    "    \n",
    "    scores = {}\n",
    "    for categoria, padroes in INDICADORES_SECAO.items():\n",
    "        score = sum(1 for p in padroes if re.search(p, paragrafo_lower))\n",
    "        scores[categoria] = score\n",
    "    \n",
    "    if max(scores.values()) == 0:\n",
    "        return 'outro'\n",
    "    \n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentar_documento(texto: str) -> List[Dict]:\n",
    "    \"\"\"Segmenta documento em par√°grafos classificados.\"\"\"\n",
    "    # Divide em par√°grafos (2+ quebras de linha ou mudan√ßa de contexto)\n",
    "    paragrafos = re.split(r'\\n\\s*\\n|\\n(?=[A-Z])', texto)\n",
    "    \n",
    "    segmentos = []\n",
    "    for i, p in enumerate(paragrafos):\n",
    "        p_limpo = p.strip()\n",
    "        if len(p_limpo) > 10:\n",
    "            categoria = classificar_paragrafo(p_limpo)\n",
    "            segmentos.append({\n",
    "                'idx': i,\n",
    "                'categoria': categoria,\n",
    "                'texto': p_limpo[:500],\n",
    "                'tamanho': len(p_limpo)\n",
    "            })\n",
    "    \n",
    "    return segmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_estrutura_corpus(df: pd.DataFrame, coluna: str = 'texto_normalizado') -> Dict:\n",
    "    \"\"\"Analisa estrutura de todos os documentos.\"\"\"\n",
    "    tipos_oficio = []\n",
    "    categorias_secao = Counter()\n",
    "    \n",
    "    for texto in df[coluna]:\n",
    "        # Tipo de of√≠cio\n",
    "        tipo = classificar_tipo_oficio(texto)\n",
    "        tipos_oficio.append(tipo['tipo'])\n",
    "        \n",
    "        # Se√ß√µes\n",
    "        segmentos = segmentar_documento(texto)\n",
    "        for seg in segmentos:\n",
    "            categorias_secao[seg['categoria']] += 1\n",
    "    \n",
    "    return {\n",
    "        'tipos_oficio': Counter(tipos_oficio),\n",
    "        'categorias_secao': categorias_secao.most_common(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estrutura = analisar_estrutura_corpus(df)\n",
    "# print(\"Tipos de of√≠cio:\", estrutura['tipos_oficio'])\n",
    "# print(\"\\nSe√ß√µes encontradas:\", estrutura['categorias_secao'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Cat√°logo de Subs√≠dios e Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOGO_SUBSIDIOS = [\n",
    "    {'id': 1, 'nome': 'extrato_conta_corrente', \n",
    "     'keywords': ['extrato', 'conta corrente', 'movimenta√ß√£o', 'lan√ßamentos', 'movimenta√ß√µes'],\n",
    "     'variantes': ['extrato banc√°rio', 'extratos de conta', 'movimenta√ß√£o financeira']},\n",
    "    \n",
    "    {'id': 2, 'nome': 'extrato_poupanca',\n",
    "     'keywords': ['poupan√ßa', 'caderneta', 'extrato poupan√ßa'],\n",
    "     'variantes': ['caderneta de poupan√ßa', 'conta poupan√ßa']},\n",
    "    \n",
    "    {'id': 3, 'nome': 'extrato_investimentos',\n",
    "     'keywords': ['investimento', 'aplica√ß√£o', 'renda fixa', 'renda vari√°vel', 'fundo', 'cdb', 'lci', 'lca'],\n",
    "     'variantes': ['aplica√ß√µes financeiras', 'fundos de investimento']},\n",
    "    \n",
    "    {'id': 4, 'nome': 'fatura_cartao',\n",
    "     'keywords': ['cart√£o de cr√©dito', 'fatura', 'cart√£o', 'transa√ß√µes cart√£o'],\n",
    "     'variantes': ['faturas de cart√£o', 'cart√µes de cr√©dito', 'despesas cart√£o']},\n",
    "    \n",
    "    {'id': 5, 'nome': 'contratos',\n",
    "     'keywords': ['contrato', 'abertura de conta', 'ades√£o', 'termos'],\n",
    "     'variantes': ['contratos banc√°rios', 'documentos de abertura']},\n",
    "    \n",
    "    {'id': 6, 'nome': 'emprestimos',\n",
    "     'keywords': ['empr√©stimo', 'financiamento', 'cr√©dito pessoal', 'consignado', 'parcelas'],\n",
    "     'variantes': ['opera√ß√µes de cr√©dito', 'financiamentos ativos']},\n",
    "    \n",
    "    {'id': 7, 'nome': 'pix',\n",
    "     'keywords': ['pix', 'chave pix', 'transfer√™ncia pix', 'recebimentos pix'],\n",
    "     'variantes': ['transa√ß√µes pix', 'chaves pix cadastradas']},\n",
    "    \n",
    "    {'id': 8, 'nome': 'ted_doc',\n",
    "     'keywords': ['ted', 'doc', 'transfer√™ncia', 'transfer√™ncias'],\n",
    "     'variantes': ['transfer√™ncias eletr√¥nicas', 'remessas']},\n",
    "    \n",
    "    {'id': 9, 'nome': 'cheques',\n",
    "     'keywords': ['cheque', 'tal√£o', 'compensa√ß√£o', 'cheques'],\n",
    "     'variantes': ['folhas de cheque', 'cheques compensados']},\n",
    "    \n",
    "    {'id': 10, 'nome': 'boletos',\n",
    "     'keywords': ['boleto', 'boletos', 't√≠tulos', 'cobran√ßa'],\n",
    "     'variantes': ['pagamentos de boletos', 'boletos emitidos']},\n",
    "    \n",
    "    {'id': 11, 'nome': 'dados_cadastrais',\n",
    "     'keywords': ['cadastro', 'dados cadastrais', 'endere√ßo', 'telefone', 'email', 'filia√ß√£o'],\n",
    "     'variantes': ['ficha cadastral', 'informa√ß√µes cadastrais', 'dados pessoais']},\n",
    "    \n",
    "    {'id': 12, 'nome': 'procuracoes',\n",
    "     'keywords': ['procura√ß√£o', 'procura√ß√µes', 'representante', 'mandat√°rio'],\n",
    "     'variantes': ['procuradores', 'representantes legais']},\n",
    "    \n",
    "    {'id': 13, 'nome': 'cofre',\n",
    "     'keywords': ['cofre', 'caixa de seguran√ßa', 'aluguel cofre'],\n",
    "     'variantes': ['cofres alugados']},\n",
    "    \n",
    "    {'id': 14, 'nome': 'previdencia',\n",
    "     'keywords': ['previd√™ncia', 'pgbl', 'vgbl', 'aposentadoria'],\n",
    "     'variantes': ['previd√™ncia privada', 'planos de previd√™ncia']},\n",
    "    \n",
    "    {'id': 15, 'nome': 'seguros',\n",
    "     'keywords': ['seguro', 'ap√≥lice', 'seguros', 'sinistro'],\n",
    "     'variantes': ['ap√≥lices de seguro', 'seguros contratados']},\n",
    "    \n",
    "    {'id': 16, 'nome': 'consorcio',\n",
    "     'keywords': ['cons√≥rcio', 'cota', 'contempla√ß√£o'],\n",
    "     'variantes': ['cotas de cons√≥rcio']},\n",
    "    \n",
    "    {'id': 17, 'nome': 'cambio',\n",
    "     'keywords': ['c√¢mbio', 'd√≥lar', 'moeda estrangeira', 'remessa internacional'],\n",
    "     'variantes': ['opera√ß√µes de c√¢mbio', 'compra e venda de moeda']},\n",
    "    \n",
    "    {'id': 18, 'nome': 'debito_automatico',\n",
    "     'keywords': ['d√©bito autom√°tico', 'agendamento', 'pagamentos recorrentes'],\n",
    "     'variantes': ['d√©bitos autom√°ticos cadastrados']},\n",
    "    \n",
    "    {'id': 19, 'nome': 'saques',\n",
    "     'keywords': ['saque', 'saques', 'retirada', 'caixa eletr√¥nico', 'atm'],\n",
    "     'variantes': ['saques realizados', 'retiradas em esp√©cie']},\n",
    "    \n",
    "    {'id': 20, 'nome': 'depositos',\n",
    "     'keywords': ['dep√≥sito', 'dep√≥sitos', 'envelope'],\n",
    "     'variantes': ['dep√≥sitos realizados', 'dep√≥sitos em esp√©cie']},\n",
    "    \n",
    "    {'id': 21, 'nome': 'saldo',\n",
    "     'keywords': ['saldo', 'posi√ß√£o', 'dispon√≠vel'],\n",
    "     'variantes': ['saldo em conta', 'posi√ß√£o de saldo']},\n",
    "    \n",
    "    {'id': 22, 'nome': 'bloqueio_judicial',\n",
    "     'keywords': ['bloqueio', 'penhora', 'bacenjud', 'sisbajud', 'indisponibilidade'],\n",
    "     'variantes': ['bloqueios judiciais', 'ordens de bloqueio']},\n",
    "    \n",
    "    {'id': 23, 'nome': 'historico_relacionamento',\n",
    "     'keywords': ['hist√≥rico', 'relacionamento', 'tempo de conta', 'abertura'],\n",
    "     'variantes': ['hist√≥rico de relacionamento', 'data de abertura']},\n",
    "    \n",
    "    {'id': 24, 'nome': 'imagens_documentos',\n",
    "     'keywords': ['imagem', 'c√≥pia', 'digitaliza√ß√£o', 'frente e verso', 'documento'],\n",
    "     'variantes': ['imagens de cheques', 'c√≥pias de documentos']},\n",
    "    \n",
    "    {'id': 25, 'nome': 'limite_credito',\n",
    "     'keywords': ['limite', 'cr√©dito aprovado', 'margem'],\n",
    "     'variantes': ['limites de cr√©dito', 'margem consign√°vel']},\n",
    "    \n",
    "    {'id': 26, 'nome': 'contas_vinculadas',\n",
    "     'keywords': ['conta vinculada', 'contas', 'todas as contas'],\n",
    "     'variantes': ['contas do titular', 'contas existentes']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_subsidios(texto: str) -> List[Dict]:\n",
    "    \"\"\"Detecta subs√≠dios mencionados no texto.\"\"\"\n",
    "    texto_lower = texto.lower()\n",
    "    encontrados = []\n",
    "    \n",
    "    for subsidio in CATALOGO_SUBSIDIOS:\n",
    "        todas_keywords = subsidio['keywords'] + subsidio.get('variantes', [])\n",
    "        matches = []\n",
    "        \n",
    "        for kw in todas_keywords:\n",
    "            if kw.lower() in texto_lower:\n",
    "                # Extrai contexto\n",
    "                idx = texto_lower.find(kw.lower())\n",
    "                ctx_inicio = max(0, idx - 40)\n",
    "                ctx_fim = min(len(texto), idx + len(kw) + 40)\n",
    "                matches.append({\n",
    "                    'keyword': kw,\n",
    "                    'contexto': texto[ctx_inicio:ctx_fim].strip()\n",
    "                })\n",
    "        \n",
    "        if matches:\n",
    "            encontrados.append({\n",
    "                'id': subsidio['id'],\n",
    "                'nome': subsidio['nome'],\n",
    "                'score': len(matches),\n",
    "                'matches': matches[:3]\n",
    "            })\n",
    "    \n",
    "    return sorted(encontrados, key=lambda x: x['score'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_termos_nao_catalogados(texto: str) -> List[str]:\n",
    "    \"\"\"Identifica poss√≠veis subs√≠dios n√£o presentes no cat√°logo.\"\"\"\n",
    "    texto_lower = texto.lower()\n",
    "    \n",
    "    # Padr√µes de solicita√ß√£o\n",
    "    padroes = [\n",
    "        r'(?:forne√ßa|informe|encaminhe|apresente|disponibilize)\\s+([^.;]{10,80})',\n",
    "        r'(?:solicito|requeiro)\\s+(?:a\\s+)?(?:apresenta√ß√£o|fornecimento)\\s+(?:de\\s+)?([^.;]{10,80})',\n",
    "        r'(?:informa√ß√µes\\s+(?:sobre|relativas?\\s+a))\\s+([^.;]{10,80})',\n",
    "    ]\n",
    "    \n",
    "    # Keywords conhecidas\n",
    "    keywords_conhecidas = set()\n",
    "    for sub in CATALOGO_SUBSIDIOS:\n",
    "        keywords_conhecidas.update(kw.lower() for kw in sub['keywords'])\n",
    "        keywords_conhecidas.update(kw.lower() for kw in sub.get('variantes', []))\n",
    "    \n",
    "    candidatos = []\n",
    "    for p in padroes:\n",
    "        for match in re.finditer(p, texto_lower):\n",
    "            trecho = match.group(1).strip()\n",
    "            # Verifica se n√£o √© conhecido\n",
    "            eh_conhecido = any(kw in trecho for kw in keywords_conhecidas)\n",
    "            if not eh_conhecido and len(trecho) > 15:\n",
    "                candidatos.append(trecho[:100])\n",
    "    \n",
    "    return list(set(candidatos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_subsidios_corpus(df: pd.DataFrame, coluna: str = 'texto_normalizado') -> Dict:\n",
    "    \"\"\"Analisa subs√≠dios em todo o corpus.\"\"\"\n",
    "    frequencia = Counter()\n",
    "    nao_catalogados = []\n",
    "    exemplos = defaultdict(list)\n",
    "    \n",
    "    for idx, texto in enumerate(df[coluna]):\n",
    "        # Subs√≠dios conhecidos\n",
    "        detectados = detectar_subsidios(texto)\n",
    "        for sub in detectados:\n",
    "            frequencia[sub['nome']] += 1\n",
    "            if len(exemplos[sub['nome']]) < 3:\n",
    "                exemplos[sub['nome']].append({\n",
    "                    'doc_idx': idx,\n",
    "                    'contexto': sub['matches'][0]['contexto'] if sub['matches'] else ''\n",
    "                })\n",
    "        \n",
    "        # N√£o catalogados\n",
    "        nao_cat = encontrar_termos_nao_catalogados(texto)\n",
    "        for termo in nao_cat:\n",
    "            nao_catalogados.append({'doc_idx': idx, 'termo': termo})\n",
    "    \n",
    "    return {\n",
    "        'frequencia': frequencia.most_common(),\n",
    "        'exemplos': dict(exemplos),\n",
    "        'nao_catalogados': nao_catalogados,\n",
    "        'total_nao_catalogados': len(nao_catalogados)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultado_subs = analisar_subsidios_corpus(df)\n",
    "# print(\"Subs√≠dios mais frequentes:\")\n",
    "# for nome, freq in resultado_subs['frequencia'][:10]:\n",
    "#     print(f\"  {nome}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Gera√ß√£o de Prompts para LLM (OpenAI/Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_prompt_classificacao_subsidio(trecho: str, nomes_catalogo: List[str] = None) -> str:\n",
    "    \"\"\"Gera prompt para LLM classificar um subs√≠dio n√£o identificado.\"\"\"\n",
    "    if nomes_catalogo is None:\n",
    "        nomes_catalogo = [s['nome'] for s in CATALOGO_SUBSIDIOS]\n",
    "    \n",
    "    return f\"\"\"Voc√™ √© um especialista em an√°lise de documentos jur√≠dicos banc√°rios brasileiros.\n",
    "\n",
    "Analise o trecho abaixo extra√≠do de um of√≠cio judicial de quebra de sigilo banc√°rio.\n",
    "Identifique qual tipo de subs√≠dio (produto/servi√ßo/informa√ß√£o) banc√°rio est√° sendo solicitado.\n",
    "\n",
    "CAT√ÅLOGO DE SUBS√çDIOS CONHECIDOS:\n",
    "{json.dumps(nomes_catalogo, indent=2, ensure_ascii=False)}\n",
    "\n",
    "TRECHO A ANALISAR:\n",
    "\"{trecho}\"\n",
    "\n",
    "RESPONDA APENAS EM JSON V√ÅLIDO:\n",
    "{{\n",
    "  \"subsidio_identificado\": \"nome do cat√°logo ou 'nao_catalogado'\",\n",
    "  \"confianca\": \"alta|media|baixa\",\n",
    "  \"justificativa\": \"explica√ß√£o breve\",\n",
    "  \"sugestao_novo_subsidio\": \"se n√£o catalogado, sugira nome padronizado\",\n",
    "  \"keywords_sugeridas\": [\"lista\", \"de\", \"keywords\"]\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_prompt_extracao_periodo(texto: str) -> str:\n",
    "    \"\"\"Gera prompt para LLM extrair per√≠odo de quebra de sigilo.\"\"\"\n",
    "    return f\"\"\"Voc√™ √© um especialista em an√°lise de documentos jur√≠dicos banc√°rios brasileiros.\n",
    "\n",
    "Extraia o per√≠odo de quebra de sigilo solicitado no texto abaixo.\n",
    "O per√≠odo pode estar expresso como:\n",
    "- Datas espec√≠ficas (01/01/2020 a 31/12/2023)\n",
    "- Per√≠odo relativo (√∫ltimos 5 anos)\n",
    "- Refer√™ncia a evento (desde a abertura da conta)\n",
    "\n",
    "TEXTO:\n",
    "\"{texto[:2000]}\"\n",
    "\n",
    "RESPONDA APENAS EM JSON V√ÅLIDO:\n",
    "{{\n",
    "  \"data_inicio\": \"DD/MM/AAAA ou null\",\n",
    "  \"data_fim\": \"DD/MM/AAAA ou null\",\n",
    "  \"periodo_relativo\": \"descri√ß√£o textual ou null\",\n",
    "  \"texto_original\": \"trecho exato do documento\",\n",
    "  \"confianca\": \"alta|media|baixa\",\n",
    "  \"observacoes\": \"qualquer nota relevante\"\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_prompt_extracao_investigados(texto: str) -> str:\n",
    "    \"\"\"Gera prompt para LLM extrair dados dos investigados.\"\"\"\n",
    "    return f\"\"\"Voc√™ √© um especialista em an√°lise de documentos jur√≠dicos banc√°rios brasileiros.\n",
    "\n",
    "Extraia as informa√ß√µes dos investigados/alvos da quebra de sigilo no texto abaixo.\n",
    "\n",
    "TEXTO:\n",
    "\"{texto[:3000]}\"\n",
    "\n",
    "RESPONDA APENAS EM JSON V√ÅLIDO:\n",
    "{{\n",
    "  \"investigados\": [\n",
    "    {{\n",
    "      \"nome\": \"nome completo\",\n",
    "      \"cpf\": \"000.000.000-00 ou null\",\n",
    "      \"cnpj\": \"00.000.000/0000-00 ou null\",\n",
    "      \"qualificacao\": \"r√©u, investigado, denunciado, etc\",\n",
    "      \"outras_info\": \"filia√ß√£o, endere√ßo, etc\"\n",
    "    }}\n",
    "  ],\n",
    "  \"total_identificados\": 0,\n",
    "  \"confianca\": \"alta|media|baixa\"\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_batch_analise_llm(df: pd.DataFrame, coluna: str = 'texto_normalizado',\n",
    "                                tipo: str = 'subsidio', max_docs: int = 10) -> List[Dict]:\n",
    "    \"\"\"Prepara batch de prompts para an√°lise via LLM.\"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    for idx, texto in enumerate(df[coluna][:max_docs]):\n",
    "        if tipo == 'subsidio':\n",
    "            nao_cat = encontrar_termos_nao_catalogados(texto)\n",
    "            for termo in nao_cat[:2]:  # Max 2 por doc\n",
    "                prompts.append({\n",
    "                    'doc_idx': idx,\n",
    "                    'tipo': 'classificacao_subsidio',\n",
    "                    'prompt': gerar_prompt_classificacao_subsidio(termo)\n",
    "                })\n",
    "        elif tipo == 'periodo':\n",
    "            prompts.append({\n",
    "                'doc_idx': idx,\n",
    "                'tipo': 'extracao_periodo',\n",
    "                'prompt': gerar_prompt_extracao_periodo(texto)\n",
    "            })\n",
    "        elif tipo == 'investigados':\n",
    "            prompts.append({\n",
    "                'doc_idx': idx,\n",
    "                'tipo': 'extracao_investigados',\n",
    "                'prompt': gerar_prompt_extracao_investigados(texto)\n",
    "            })\n",
    "    \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo:\n",
    "# prompts = preparar_batch_analise_llm(df, tipo='subsidio', max_docs=5)\n",
    "# print(f\"Gerados {len(prompts)} prompts\")\n",
    "# print(\"\\nExemplo de prompt:\")\n",
    "# print(prompts[0]['prompt'][:500] if prompts else 'Nenhum prompt gerado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Pipeline Completo de An√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_analise_completa(caminho_excel: str, coluna_texto: str,\n",
    "                               usar_spellcheck: bool = False) -> Dict:\n",
    "    \"\"\"Executa pipeline completo de an√°lise.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"AN√ÅLISE DE OF√çCIOS DE QUEBRA DE SIGILO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Carregamento\n",
    "    print(\"\\n[1/7] Carregando dados...\")\n",
    "    df = carregar_oficios(caminho_excel, coluna_texto)\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # 2. Diagn√≥stico de encoding\n",
    "    print(\"\\n[2/7] Diagnosticando encoding...\")\n",
    "    encoding_stats = analisar_encoding_corpus(df, 'texto_original')\n",
    "    print(f\"  Docs com mojibake: {encoding_stats['docs_com_mojibake']}\")\n",
    "    print(f\"  Palavras hifenizadas: {encoding_stats['total_hifenizacao']}\")\n",
    "    \n",
    "    # 3. Normaliza√ß√£o\n",
    "    print(\"\\n[3/7] Normalizando textos...\")\n",
    "    df = normalizar_corpus(df, 'texto_original')\n",
    "    \n",
    "    # 4. Qualidade OCR\n",
    "    print(\"\\n[4/7] Avaliando qualidade OCR...\")\n",
    "    df = analisar_qualidade_corpus(df, 'texto_normalizado', usar_spellcheck)\n",
    "    \n",
    "    # 5. Men√ß√µes Ita√∫\n",
    "    print(\"\\n[5/7] Detectando men√ß√µes ao Ita√∫...\")\n",
    "    itau_stats = analisar_mencoes_itau_corpus(df, 'texto_normalizado')\n",
    "    print(f\"  Docs mencionando Ita√∫: {itau_stats['docs_com_itau']}\")\n",
    "    print(f\"  Varia√ß√µes encontradas: {len(itau_stats['variacoes_nome'])}\")\n",
    "    \n",
    "    # 6. An√°lise estrutural\n",
    "    print(\"\\n[6/7] Analisando estrutura...\")\n",
    "    estrutura = analisar_estrutura_corpus(df, 'texto_normalizado')\n",
    "    print(f\"  Tipos de of√≠cio: {dict(estrutura['tipos_oficio'])}\")\n",
    "    \n",
    "    # 7. Subs√≠dios\n",
    "    print(\"\\n[7/7] Detectando subs√≠dios...\")\n",
    "    subsidios = analisar_subsidios_corpus(df, 'texto_normalizado')\n",
    "    print(f\"  Subs√≠dios distintos: {len(subsidios['frequencia'])}\")\n",
    "    print(f\"  Termos n√£o catalogados: {subsidios['total_nao_catalogados']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AN√ÅLISE CONCLU√çDA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'dataframe': df,\n",
    "        'encoding': encoding_stats,\n",
    "        'itau': itau_stats,\n",
    "        'estrutura': estrutura,\n",
    "        'subsidios': subsidios\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXECUTAR AN√ÅLISE ===\n",
    "# resultado = executar_analise_completa('seu_arquivo.xlsx', 'coluna_texto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Exporta√ß√£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_resultados(resultado: Dict, caminho_saida: str = 'analise_oficios_resultado.xlsx'):\n",
    "    \"\"\"Exporta resultados para Excel.\"\"\"\n",
    "    df = resultado['dataframe']\n",
    "    \n",
    "    with pd.ExcelWriter(caminho_saida, engine='openpyxl') as writer:\n",
    "        # Aba principal com documentos\n",
    "        colunas_export = ['texto_original', 'texto_normalizado', 'ocr_score', \n",
    "                         'ocr_qualidade', 'ocr_requer_revisao', 'ocr_total_tokens']\n",
    "        colunas_disponiveis = [c for c in colunas_export if c in df.columns]\n",
    "        df[colunas_disponiveis].to_excel(writer, sheet_name='Documentos', index=True)\n",
    "        \n",
    "        # Frequ√™ncia de subs√≠dios\n",
    "        df_subs = pd.DataFrame(resultado['subsidios']['frequencia'], \n",
    "                               columns=['subsidio', 'frequencia'])\n",
    "        df_subs.to_excel(writer, sheet_name='Subsidios', index=False)\n",
    "        \n",
    "        # Termos n√£o catalogados\n",
    "        df_nao_cat = pd.DataFrame(resultado['subsidios']['nao_catalogados'])\n",
    "        df_nao_cat.to_excel(writer, sheet_name='Nao_Catalogados', index=False)\n",
    "        \n",
    "        # Varia√ß√µes Ita√∫\n",
    "        df_itau = pd.DataFrame(resultado['itau']['variacoes_nome'],\n",
    "                               columns=['variacao', 'frequencia'])\n",
    "        df_itau.to_excel(writer, sheet_name='Variacoes_Itau', index=False)\n",
    "        \n",
    "        # Resumo\n",
    "        resumo = {\n",
    "            'M√©trica': [\n",
    "                'Total documentos',\n",
    "                'Score OCR m√©dio',\n",
    "                'Docs para revis√£o',\n",
    "                'Docs com Ita√∫',\n",
    "                'Docs com mojibake',\n",
    "                'Palavras hifenizadas',\n",
    "                'Termos n√£o catalogados'\n",
    "            ],\n",
    "            'Valor': [\n",
    "                len(df),\n",
    "                df['ocr_score'].mean() if 'ocr_score' in df.columns else 'N/A',\n",
    "                df['ocr_requer_revisao'].sum() if 'ocr_requer_revisao' in df.columns else 'N/A',\n",
    "                resultado['itau']['docs_com_itau'],\n",
    "                resultado['encoding']['docs_com_mojibake'],\n",
    "                resultado['encoding']['total_hifenizacao'],\n",
    "                resultado['subsidios']['total_nao_catalogados']\n",
    "            ]\n",
    "        }\n",
    "        pd.DataFrame(resumo).to_excel(writer, sheet_name='Resumo', index=False)\n",
    "    \n",
    "    print(f\"‚úì Resultados exportados para {caminho_saida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar_resultados(resultado, 'analise_oficios_resultado.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
